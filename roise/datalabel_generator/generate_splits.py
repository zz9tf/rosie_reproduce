#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„æ•°æ®é›†åˆ†å‰²è„šæœ¬ - ç›´æ¥ç”Ÿæˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†æ–‡ä»¶
"""

import argparse
import numpy as np
import pandas as pd
import os

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç”Ÿæˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†æ•°æ®é›†æ–‡ä»¶")
    parser.add_argument("--data-file", required=True, help="æ•°æ®parquetæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--split-ratios", nargs=3, type=float, default=[0.8, 0.1, 0.1], 
                        help="è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†æ¯”ä¾‹ [train val test]")
    parser.add_argument("--split-seed", type=int, default=42, help="æ•°æ®åˆ†å‰²éšæœºç§å­")
    parser.add_argument("--max-samples", type=int, default=None, help="é™åˆ¶ä½¿ç”¨çš„æœ€å¤§æ ·æœ¬æ•°ï¼ˆé»˜è®¤ä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼‰")
    parser.add_argument("--shuffle", action="store_true", default=True, help="æ˜¯å¦æ‰“ä¹±æ•°æ®é¡ºåºï¼ˆé»˜è®¤å¼€å¯ï¼‰")
    parser.add_argument("--no-shuffle", action="store_false", dest="shuffle", help="ä¸æ‰“ä¹±æ•°æ®é¡ºåº")
    parser.add_argument("--output-dir", type=str, default="./splits", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--output-format", choices=['parquet', 'csv'], default='parquet', help="è¾“å‡ºæ–‡ä»¶æ ¼å¼")
    
    args = parser.parse_args()
    
    print("ğŸš€ æ•°æ®é›†åˆ†å‰²ç”Ÿæˆå™¨")
    print("=" * 50)
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {args.data_file}")
    print(f"ğŸ“Š åˆ†å‰²æ¯”ä¾‹: {args.split_ratios}")
    print(f"ğŸ² éšæœºç§å­: {args.split_seed}")
    print(f"ğŸ“ æœ€å¤§æ ·æœ¬æ•°: {args.max_samples if args.max_samples else 'æ— é™åˆ¶'}")
    print(f"ğŸ”€ æ•°æ®æ‰“ä¹±: {'æ˜¯' if args.shuffle else 'å¦'}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ“„ è¾“å‡ºæ ¼å¼: {args.output_format}")
    print()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ç”Ÿæˆåˆ†å‰²
    print("ğŸ“Š ç”Ÿæˆæ•°æ®é›†åˆ†å‰²...")
    
    # è¯»å–æ•°æ®æ–‡ä»¶è·å–æ€»æ ·æœ¬æ•°
    df = pd.read_parquet(args.data_file)
    original_total_samples = len(df)
    
    # åº”ç”¨æœ€å¤§æ ·æœ¬æ•°é™åˆ¶
    if args.max_samples is not None:
        if args.max_samples <= 0:
            raise ValueError(f"æœ€å¤§æ ·æœ¬æ•°å¿…é¡»å¤§äº0ï¼Œå½“å‰ä¸º: {args.max_samples}")
        if args.max_samples > original_total_samples:
            print(f"âš ï¸  è­¦å‘Š: æŒ‡å®šçš„æœ€å¤§æ ·æœ¬æ•°({args.max_samples})è¶…è¿‡å®é™…æ ·æœ¬æ•°({original_total_samples})ï¼Œå°†ä½¿ç”¨å…¨éƒ¨æ ·æœ¬")
            total_samples = original_total_samples
        else:
            total_samples = args.max_samples
    else:
        total_samples = original_total_samples
    
    print(f"ğŸ“ˆ åŸå§‹æ ·æœ¬æ•°: {original_total_samples}")
    print(f"ğŸ“ˆ ä½¿ç”¨æ ·æœ¬æ•°: {total_samples}")
    print(f"ğŸ“Š åˆ†å‰²æ¯”ä¾‹: è®­ç»ƒ={args.split_ratios[0]:.1%}, éªŒè¯={args.split_ratios[1]:.1%}, æµ‹è¯•={args.split_ratios[2]:.1%}")
    
    # éªŒè¯åˆ†å‰²æ¯”ä¾‹
    if abs(sum(args.split_ratios) - 1.0) > 1e-6:
        raise ValueError(f"åˆ†å‰²æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º: {sum(args.split_ratios)}")
    
    # ç”Ÿæˆæ‰€æœ‰ç´¢å¼•
    all_indices = np.arange(original_total_samples)
    
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
    np.random.seed(args.split_seed)
    
    # æ ¹æ®shuffleå‚æ•°å†³å®šæ˜¯å¦æ‰“ä¹±æ•°æ®
    if args.shuffle:
        np.random.shuffle(all_indices)
        print("ğŸ”€ æ‰€æœ‰æ•°æ®å·²æ‰“ä¹±")
    else:
        print("ğŸ“‹ ä¿æŒæ•°æ®åŸå§‹é¡ºåº")
    
    # ä»æ‰“ä¹±åçš„ç»“æœä¸­å–æŒ‡å®šæ•°é‡çš„æ ·æœ¬
    if args.max_samples is not None and args.max_samples < original_total_samples:
        indices = all_indices[:args.max_samples]
        print(f"ğŸ“ ä»æ‰“ä¹±åçš„æ•°æ®ä¸­å–å‰ {args.max_samples} ä¸ªæ ·æœ¬")
    else:
        indices = all_indices
    
    # è®¡ç®—åˆ†å‰²ç‚¹
    train_size = int(total_samples * args.split_ratios[0])
    val_size = int(total_samples * args.split_ratios[1])
    
    # åˆ†å‰²ç´¢å¼•
    train_indices = indices[:train_size]
    val_indices = indices[train_size:train_size + val_size]
    test_indices = indices[train_size + val_size:]
    
    # æ ¹æ®ç´¢å¼•åˆ†å‰²æ•°æ®
    train_df = df.iloc[train_indices].reset_index(drop=True)
    val_df = df.iloc[val_indices].reset_index(drop=True)
    test_df = df.iloc[test_indices].reset_index(drop=True)
    
    print(f"âœ… è®­ç»ƒé›†: {len(train_df)} ä¸ªæ ·æœ¬")
    print(f"âœ… éªŒè¯é›†: {len(val_df)} ä¸ªæ ·æœ¬")
    print(f"âœ… æµ‹è¯•é›†: {len(test_df)} ä¸ªæ ·æœ¬")
    
    # ä¿å­˜æ•°æ®é›†æ–‡ä»¶
    file_extension = f".{args.output_format}"
    
    train_file = os.path.join(args.output_dir, f"train{file_extension}")
    val_file = os.path.join(args.output_dir, f"val{file_extension}")
    test_file = os.path.join(args.output_dir, f"test{file_extension}")
    
    if args.output_format == 'parquet':
        train_df.to_parquet(train_file, index=False)
        val_df.to_parquet(val_file, index=False)
        test_df.to_parquet(test_file, index=False)
    else:  # csv
        train_df.to_csv(train_file, index=False)
        val_df.to_csv(val_file, index=False)
        test_df.to_csv(test_file, index=False)
    
    print(f"ğŸ’¾ è®­ç»ƒé›†å·²ä¿å­˜åˆ°: {train_file}")
    print(f"ğŸ’¾ éªŒè¯é›†å·²ä¿å­˜åˆ°: {val_file}")
    print(f"ğŸ’¾ æµ‹è¯•é›†å·²ä¿å­˜åˆ°: {test_file}")
    
    # ä¿å­˜åˆ†å‰²ä¿¡æ¯å…ƒæ•°æ®
    metadata_file = os.path.join(args.output_dir, "split_metadata.txt")
    with open(metadata_file, 'w', encoding='utf-8') as f:
        f.write("æ•°æ®é›†åˆ†å‰²ä¿¡æ¯\n")
        f.write("=" * 30 + "\n")
        f.write(f"åŸå§‹æ•°æ®æ–‡ä»¶: {args.data_file}\n")
        f.write(f"åŸå§‹æ ·æœ¬æ•°: {original_total_samples}\n")
        f.write(f"ä½¿ç”¨æ ·æœ¬æ•°: {total_samples}\n")
        f.write(f"æœ€å¤§æ ·æœ¬æ•°é™åˆ¶: {args.max_samples if args.max_samples else 'æ— é™åˆ¶'}\n")
        f.write(f"éšæœºç§å­: {args.split_seed}\n")
        f.write(f"æ•°æ®æ‰“ä¹±: {'æ˜¯' if args.shuffle else 'å¦'}\n")
        f.write(f"åˆ†å‰²æ¯”ä¾‹: {args.split_ratios}\n")
        f.write(f"è¾“å‡ºæ ¼å¼: {args.output_format}\n")
        f.write("\nåˆ†å‰²ç»“æœ:\n")
        f.write(f"è®­ç»ƒé›†: {len(train_df)} ä¸ªæ ·æœ¬ ({len(train_df)/total_samples:.1%})\n")
        f.write(f"éªŒè¯é›†: {len(val_df)} ä¸ªæ ·æœ¬ ({len(val_df)/total_samples:.1%})\n")
        f.write(f"æµ‹è¯•é›†: {len(test_df)} ä¸ªæ ·æœ¬ ({len(test_df)/total_samples:.1%})\n")
    
    print(f"ğŸ“‹ åˆ†å‰²å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {metadata_file}")
    
    print("\nğŸ‰ æ•°æ®é›†åˆ†å‰²å®Œæˆï¼")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   - è®­ç»ƒé›†: {len(train_df)} ä¸ªæ ·æœ¬ ({len(train_df)/total_samples:.1%})")
    print(f"   - éªŒè¯é›†: {len(val_df)} ä¸ªæ ·æœ¬ ({len(val_df)/total_samples:.1%})")
    print(f"   - æµ‹è¯•é›†: {len(test_df)} ä¸ªæ ·æœ¬ ({len(test_df)/total_samples:.1%})")
    print(f"   - ä½¿ç”¨æ ·æœ¬æ•°: {total_samples}")
    if args.max_samples is not None:
        print(f"   - åŸå§‹æ ·æœ¬æ•°: {original_total_samples}")
        print(f"   - æ ·æœ¬æ•°é™åˆ¶: {args.max_samples}")
    print(f"   - éšæœºç§å­: {args.split_seed}")
    print(f"   - æ•°æ®æ‰“ä¹±: {'æ˜¯' if args.shuffle else 'å¦'}")
    print(f"   - åˆ†å‰²æ¯”ä¾‹: {args.split_ratios}")
    print(f"   - è¾“å‡ºæ ¼å¼: {args.output_format}")
    print(f"   - è¾“å‡ºç›®å½•: {args.output_dir}")

if __name__ == "__main__":
    main()
