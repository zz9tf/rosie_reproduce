#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„è®­ç»ƒè„šæœ¬ - é™åˆ¶æ•°æ®é›†å¤§å°å¹¶æ”¹è¿›æ€§èƒ½
"""

import argparse
import os
from patch_dataset import PatchImageDataset, get_default_transforms, collate_fn
from torch.utils.data import DataLoader, Subset
from model import ProteinPredictor

def main():
    """ä¼˜åŒ–çš„è®­ç»ƒä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–çš„åŸºäºPatchçš„H&Eå›¾åƒè®­ç»ƒè„šæœ¬")
    parser.add_argument("--root", required=True, help="é¡¹ç›®æ ¹ç›®å½•")
    parser.add_argument("--data-file", required=True, help="æ•°æ®parquetæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--target-biomarkers", nargs="+", default=['HE', 'CD3', 'CD8'], help="ç›®æ ‡ç”Ÿç‰©æ ‡è®°ç‰©")
    parser.add_argument("--batch-size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--eval-interval", type=int, default=1000, help="éªŒè¯é—´éš”")
    parser.add_argument("--patience", type=int, default=5000, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--num-workers", type=int, default=4, help="æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--patch-size", type=int, default=128, help="patchå¤§å°")
    parser.add_argument("--use-zarr", action="store_true", default=True, help="ä½¿ç”¨zarrç›´æ¥åŠ è½½")
    parser.add_argument("--zarr-marker", type=str, default="HE", help="zarr markeråç§°")
    parser.add_argument("--split-file", required=True, help="é¢„ç”Ÿæˆçš„ç´¢å¼•åˆ†å‰²æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–è®­ç»ƒ")
    print("=" * 60)
    print(f"ğŸš€ æ•°æ®åŠ è½½æ–¹å¼: {'Zarrç›´æ¥åŠ è½½' if args.use_zarr else 'å›¾åƒæ–‡ä»¶åŠ è½½'}")
    print(f"ğŸ¯ Zarr marker: {args.zarr_marker}")
    
    # åŠ è½½åˆ†å‰²æ–‡ä»¶
    print(f"\nğŸ“‚ åŠ è½½åˆ†å‰²æ–‡ä»¶: {args.split_file}")
    import numpy as np
    data = np.load(args.split_file)
    train_indices = data['train_indices']
    val_indices = data['val_indices']
    test_indices = data['test_indices']
    
    print(f"ğŸ“Š åŸå§‹åˆ†å‰²:")
    print(f"   - è®­ç»ƒé›†: {len(train_indices)} ä¸ªæ ·æœ¬")
    print(f"   - éªŒè¯é›†: {len(val_indices)} ä¸ªæ ·æœ¬")
    print(f"   - æµ‹è¯•é›†: {len(test_indices)} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºå®Œæ•´æ•°æ®é›†
    print("\nğŸ“¦ åˆ›å»ºå®Œæ•´æ•°æ®é›†...")
    full_dataset = PatchImageDataset(
        parquet_path=args.data_file,
        patch_size=args.patch_size,
        transform=get_default_transforms()[0],
        cache_images=True,
        target_biomarkers=args.target_biomarkers,
        use_zarr=args.use_zarr,
        zarr_marker=args.zarr_marker,
    )
    
    # åº”ç”¨ç´¢å¼•åˆ†å‰²
    print("\nğŸ“¦ åº”ç”¨ç´¢å¼•åˆ†å‰²...")
    train_dataset = Subset(full_dataset, train_indices)
    val_dataset = Subset(full_dataset, val_indices)
    test_dataset = Subset(full_dataset, test_indices)
    
    print(f"âœ… è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
    print(f"âœ… éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset)}")
    print(f"âœ… æµ‹è¯•æ•°æ®é›†å¤§å°: {len(test_dataset)}")
    
    # åˆ›å»ºDataLoader
    print("\nğŸ”„ åˆ›å»ºDataLoader...")
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,  # å¯ç”¨pin_memoryåŠ é€ŸGPUä¼ è¾“
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size * 2,  # éªŒè¯æ—¶ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
        shuffle=False,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    
    print(f"âœ… è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"âœ… éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("\nğŸ¤– åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = ProteinPredictor(
        root_dir=args.root,
        train_loader=train_loader,
        val_loader=val_loader,
        learning_rate=args.lr,
        eval_interval=args.eval_interval,
        patience=args.patience,
    )
    
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {trainer.device}")
    print(f"ğŸ“Š æ¨¡å‹è¾“å‡ºç»´åº¦: {trainer.model.classifier[2].out_features}")
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸƒ å¼€å§‹è®­ç»ƒ...")
    print(f"ğŸ“Š éªŒè¯é—´éš”: æ¯ {args.eval_interval} æ­¥")
    print(f"â° æ—©åœè€å¿ƒå€¼: {args.patience} æ­¥")
    
    trainer.train()
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()


