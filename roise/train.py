#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„è®­ç»ƒè„šæœ¬ - ç›´æ¥ä½¿ç”¨åˆ†å‰²åçš„æ•°æ®é›†æ–‡ä»¶
"""

import argparse
import os
from patch_dataset import PatchImageDataset, get_default_transforms, collate_fn
from torch.utils.data import DataLoader
from model import ProteinPredictor

def main():
    """ä¼˜åŒ–çš„è®­ç»ƒä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–çš„åŸºäºPatchçš„H&Eå›¾åƒè®­ç»ƒè„šæœ¬")
    parser.add_argument("--root", required=True, help="é¡¹ç›®æ ¹ç›®å½•")
    parser.add_argument("--data-file", required=True, help="æ•°æ®parquetæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--target-biomarkers", nargs="+", default=['HE', 'CD3', 'CD8'], help="ç›®æ ‡ç”Ÿç‰©æ ‡è®°ç‰©")
    parser.add_argument("--batch-size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--eval-interval", type=int, default=1000, help="éªŒè¯é—´éš”")
    parser.add_argument("--patience", type=int, default=5000, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--num-workers", type=int, default=4, help="æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--patch-size", type=int, default=128, help="patchå¤§å°")
    parser.add_argument("--use-zarr", action="store_true", default=True, help="ä½¿ç”¨zarrç›´æ¥åŠ è½½")
    parser.add_argument("--zarr-marker", type=str, default="HE", help="zarr markeråç§°")
    parser.add_argument("--splits-dir", type=str, default="./splits", help="åˆ†å‰²æ•°æ®é›†ç›®å½•")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–è®­ç»ƒ")
    print("=" * 60)
    print(f"ğŸš€ æ•°æ®åŠ è½½æ–¹å¼: {'Zarrç›´æ¥åŠ è½½' if args.use_zarr else 'å›¾åƒæ–‡ä»¶åŠ è½½'}")
    print(f"ğŸ¯ Zarr marker: {args.zarr_marker}")
    print(f"ğŸ“ åˆ†å‰²æ•°æ®é›†ç›®å½•: {args.splits_dir}")
    
    # æ£€æŸ¥åˆ†å‰²æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    train_file = os.path.join(args.splits_dir, "train.parquet")
    val_file = os.path.join(args.splits_dir, "val.parquet")
    test_file = os.path.join(args.splits_dir, "test.parquet")
    
    if not os.path.exists(train_file):
        raise FileNotFoundError(f"è®­ç»ƒé›†æ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
    if not os.path.exists(val_file):
        raise FileNotFoundError(f"éªŒè¯é›†æ–‡ä»¶ä¸å­˜åœ¨: {val_file}")
    
    print(f"\nğŸ“‚ ä½¿ç”¨åˆ†å‰²æ•°æ®é›†:")
    print(f"   - è®­ç»ƒé›†: {train_file}")
    print(f"   - éªŒè¯é›†: {val_file}")
    if os.path.exists(test_file):
        print(f"   - æµ‹è¯•é›†: {test_file}")
    
    # åˆ›å»ºæ•°æ®é›†
    print("\nğŸ“¦ åˆ›å»ºæ•°æ®é›†...")
    transform_train, transform_eval = get_default_transforms()
    
    train_dataset = PatchImageDataset(
        parquet_path=train_file,
        patch_size=args.patch_size,
        transform=transform_train,
        cache_images=False,  # è®­ç»ƒæ—¶ä¸ç¼“å­˜ï¼ŒèŠ‚çœå†…å­˜
        target_biomarkers=args.target_biomarkers,
        use_zarr=args.use_zarr,
        zarr_marker=args.zarr_marker,
    )
    
    val_dataset = PatchImageDataset(
        parquet_path=val_file,
        patch_size=args.patch_size,
        transform=transform_eval,
        cache_images=True,  # éªŒè¯æ—¶ç¼“å­˜ï¼Œæé«˜é€Ÿåº¦
        target_biomarkers=args.target_biomarkers,
        use_zarr=args.use_zarr,
        zarr_marker=args.zarr_marker,
    )
    
    print(f"âœ… è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
    print(f"âœ… éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset)}")
    
    # åˆ›å»ºDataLoader
    print("\nğŸ”„ åˆ›å»ºDataLoader...")
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,  # å¯ç”¨pin_memoryåŠ é€ŸGPUä¼ è¾“
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size * 2,  # éªŒè¯æ—¶ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
        shuffle=False,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    
    print(f"âœ… è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"âœ… éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("\nğŸ¤– åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = ProteinPredictor(
        root_dir=args.root,
        train_loader=train_loader,
        val_loader=val_loader,
        learning_rate=args.lr,
        eval_interval=args.eval_interval,
        patience=args.patience,
    )
    
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {trainer.device}")
    print(f"ğŸ“Š æ¨¡å‹è¾“å‡ºç»´åº¦: {trainer.model.classifier[2].out_features}")
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸƒ å¼€å§‹è®­ç»ƒ...")
    print(f"ğŸ“Š éªŒè¯é—´éš”: æ¯ {args.eval_interval} æ­¥")
    print(f"â° æ—©åœè€å¿ƒå€¼: {args.patience} æ­¥")
    
    trainer.train()
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()


