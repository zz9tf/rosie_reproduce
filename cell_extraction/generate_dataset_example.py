#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ROSIEæ•°æ®é›†ç”Ÿæˆç¤ºä¾‹è„šæœ¬ ğŸ“Š
æ¼”ç¤ºå¦‚ä½•ä»SVSæ–‡ä»¶å’ŒCODEXæ•°æ®ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒæ•°æ®é›†

ä½¿ç”¨æ–¹æ³•:
    python generate_dataset_example.py --codex-dir /path/to/codex --svs-dir /path/to/svs --output-dir /path/to/output
"""

import os
import sys
import pandas as pd
import numpy as np
import pickle
import json
import uuid
from pathlib import Path
from typing import Dict, List, Tuple
import argparse
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ROSIEæ ‡å‡†50ä¸ªç”Ÿç‰©æ ‡è®°ç‰©
BIOMARKERS = [
    "DAPI", "CD45", "CD68", "CD14", "PD1", "FoxP3", "CD8", "HLA-DR", 
    "PanCK", "CD3e", "CD4", "aSMA", "CD31", "Vimentin", "CD45RO", 
    "Ki67", "CD20", "CD11c", "Podoplanin", "PDL1", "GranzymeB", 
    "CD38", "CD141", "CD21", "CD163", "BCL2", "LAG3", "EpCAM", 
    "CD44", "ICOS", "GATA3", "Gal3", "CD39", "CD34", "TIGIT", 
    "ECad", "CD40", "VISTA", "HLA-A", "MPO", "PCNA", "ATM", 
    "TP63", "IFNg", "Keratin8/18", "IDO1", "CD79a", "HLA-E", 
    "CollagenIV", "CD66"
]

class ROSIEDatasetGenerator:
    """ROSIEæ•°æ®é›†ç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str):
        """
        åˆå§‹åŒ–æ•°æ®é›†ç”Ÿæˆå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.output_dir = Path(output_dir)
        self.setup_directories()
        
    def setup_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        directories = [
            self.output_dir / "data",
            self.output_dir / "images", 
            self.output_dir / "metadata",
            self.output_dir / "runs"
        ]
        
        for dir_path in directories:
            dir_path.mkdir(parents=True, exist_ok=True)
            
        print(f"âœ… åˆ›å»ºç›®å½•ç»“æ„: {self.output_dir}")
    
    def convert_svs_to_zarr(self, svs_dir: str) -> Dict[str, str]:
        """
        å°†SVSæ–‡ä»¶è½¬æ¢ä¸ºZarræ ¼å¼
        
        Args:
            svs_dir: SVSæ–‡ä»¶ç›®å½•
            
        Returns:
            Dict mapping SVS filename to generated UUID
        """
        print("ğŸ”„ å¼€å§‹SVSåˆ°Zarrè½¬æ¢...")
        
        svs_files = list(Path(svs_dir).glob("*.svs"))
        if not svs_files:
            print("âŒ æœªæ‰¾åˆ°SVSæ–‡ä»¶")
            return {}
            
        svs_to_uuid = {}
        
        for svs_file in tqdm(svs_files, desc="è½¬æ¢SVSæ–‡ä»¶"):
            try:
                # ç”Ÿæˆå”¯ä¸€UUID
                region_uuid = str(uuid.uuid4())
                
                # åˆ›å»ºè¾“å‡ºç›®å½•
                zarr_dir = self.output_dir / "images" / region_uuid
                zarr_dir.mkdir(parents=True, exist_ok=True)
                
                # æ¨¡æ‹Ÿè½¬æ¢è¿‡ç¨‹ (å®é™…åº”è°ƒç”¨svs_to_zarr_converter)
                print(f"  è½¬æ¢: {svs_file.name} â†’ {region_uuid}")
                
                # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„è½¬æ¢å‡½æ•°:
                # from svs_to_zarr_converter import SVSToZarrConverter
                # converter = SVSToZarrConverter(str(zarr_dir))
                # converter.convert_svs_to_zarr(str(svs_file), "image")
                
                # åˆ›å»ºæ¨¡æ‹Ÿçš„zarrç»“æ„
                self._create_mock_zarr(zarr_dir / "image.ome.zarr")
                
                svs_to_uuid[svs_file.stem] = region_uuid
                
            except Exception as e:
                print(f"âŒ è½¬æ¢å¤±è´¥ {svs_file.name}: {e}")
                
        print(f"âœ… è½¬æ¢å®Œæˆ: {len(svs_to_uuid)} ä¸ªæ–‡ä»¶")
        return svs_to_uuid
    
    def _create_mock_zarr(self, zarr_path: Path):
        """åˆ›å»ºæ¨¡æ‹Ÿçš„Zarrç»“æ„ (ç”¨äºæ¼”ç¤º)"""
        zarr_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºåŸºæœ¬çš„zarrç»“æ„
        (zarr_path / ".zgroup").touch()
        (zarr_path / ".zattrs").touch()
        
        # åˆ›å»ºRGBé€šé“ç›®å½•
        for channel in [0, 1, 2]:
            channel_dir = zarr_path / str(channel)
            channel_dir.mkdir(exist_ok=True)
            (channel_dir / ".zarray").touch()
            (channel_dir / "0.0.0").touch()  # ç¤ºä¾‹æ•°æ®å—
    
    def load_codex_data(self, codex_dir: str) -> pd.DataFrame:
        """
        åŠ è½½CODEXç»†èƒæ•°æ®
        
        Args:
            codex_dir: CODEXæ•°æ®ç›®å½•
            
        Returns:
            åŒ…å«ç»†èƒæµ‹é‡æ•°æ®çš„DataFrame
        """
        print("ğŸ“Š åŠ è½½CODEXæ•°æ®...")
        
        # æ¨¡æ‹ŸåŠ è½½å¤šä¸ªCODEXå®éªŒçš„æ•°æ®
        all_data = []
        
        # å‡è®¾æœ‰å¤šä¸ªCSVæ–‡ä»¶æˆ–parquetæ–‡ä»¶
        data_files = list(Path(codex_dir).glob("*.csv")) + list(Path(codex_dir).glob("*.pqt"))
        
        if not data_files:
            print("âš ï¸ æœªæ‰¾åˆ°CODEXæ•°æ®æ–‡ä»¶ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
            return self._generate_mock_codex_data()
        
        for data_file in tqdm(data_files, desc="åŠ è½½CODEXæ–‡ä»¶"):
            try:
                if data_file.suffix == '.csv':
                    df = pd.read_csv(data_file)
                elif data_file.suffix == '.pqt':
                    df = pd.read_parquet(data_file)
                else:
                    continue
                    
                # æ·»åŠ æ•°æ®æºä¿¡æ¯
                df['source_file'] = data_file.stem
                all_data.append(df)
                
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {data_file.name}: {e}")
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            print(f"âœ… åŠ è½½å®Œæˆ: {len(combined_df)} ä¸ªç»†èƒ")
            return combined_df
        else:
            return self._generate_mock_codex_data()
    
    def _generate_mock_codex_data(self, n_cells: int = 100000) -> pd.DataFrame:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„CODEXæ•°æ®ç”¨äºæ¼”ç¤º"""
        print(f"ğŸ­ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: {n_cells} ä¸ªç»†èƒ")
        
        np.random.seed(42)
        
        data = {
            'cell_id': range(n_cells),
            'X': np.random.randint(100, 20000, n_cells),
            'Y': np.random.randint(100, 15000, n_cells),
            'study_name': np.random.choice(['Training-1', 'Training-2', 'Stanford-PGC'], n_cells),
            'tissue_type': np.random.choice(['Pancreas', 'Liver', 'Tonsil'], n_cells),
        }
        
        # ç”Ÿæˆç”Ÿç‰©æ ‡è®°ç‰©è¡¨è¾¾æ•°æ®
        for marker in BIOMARKERS:
            if marker == 'DAPI':
                # DAPIé€šå¸¸è¡¨è¾¾è¾ƒé«˜
                data[marker] = np.random.gamma(3, 0.5, n_cells)
            elif marker in ['CD45', 'CD3e', 'CD4', 'CD8']:
                # å…ç–«ç»†èƒæ ‡è®°ï¼Œéƒ¨åˆ†ç»†èƒé«˜è¡¨è¾¾
                data[marker] = np.random.beta(1, 3, n_cells) * np.random.choice([0, 1], n_cells, p=[0.7, 0.3])
            elif marker in ['Ki67', 'PCNA']:
                # å¢æ®–æ ‡è®°ï¼Œå°‘æ•°ç»†èƒè¡¨è¾¾
                data[marker] = np.random.beta(1, 5, n_cells) * np.random.choice([0, 1], n_cells, p=[0.9, 0.1])
            else:
                # å…¶ä»–æ ‡è®°ç‰©ï¼Œä¸€èˆ¬åˆ†å¸ƒ
                data[marker] = np.random.beta(2, 5, n_cells)
        
        return pd.DataFrame(data)
    
    def process_cell_data(self, codex_df: pd.DataFrame, svs_to_uuid: Dict[str, str]) -> pd.DataFrame:
        """
        å¤„ç†ç»†èƒæ•°æ®ï¼Œæ·»åŠ å¿…éœ€å­—æ®µ
        
        Args:
            codex_df: CODEXæ•°æ®DataFrame
            svs_to_uuid: SVSæ–‡ä»¶åˆ°UUIDçš„æ˜ å°„
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        print("ğŸ”§ å¤„ç†ç»†èƒæ•°æ®...")
        
        # ç”ŸæˆCODEX_ACQUISITION_ID
        unique_studies = codex_df['study_name'].unique() if 'study_name' in codex_df.columns else ['study_1']
        
        acq_ids = []
        he_coverslip_ids = []
        
        for idx, row in codex_df.iterrows():
            study = row.get('study_name', 'study_1')
            acq_id = f"{study}_{idx // 1000:03d}"  # æ¯1000ä¸ªç»†èƒä¸€ä¸ªè·å–ID
            he_id = f"HE_{study}_{idx // 2000:03d}"  # æ¯2000ä¸ªç»†èƒä¸€ä¸ªåˆ‡ç‰‡ID
            
            acq_ids.append(acq_id)
            he_coverslip_ids.append(he_id)
        
        codex_df['CODEX_ACQUISITION_ID'] = acq_ids
        codex_df['HE_COVERSLIP_ID'] = he_coverslip_ids
        
        # ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€çš„ç”Ÿç‰©æ ‡è®°ç‰©
        for marker in BIOMARKERS:
            if marker not in codex_df.columns:
                print(f"âš ï¸ ç¼ºå°‘æ ‡è®°ç‰© {marker}ï¼Œè®¾ç½®ä¸ºNaN")
                codex_df[marker] = np.nan
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        required_cols = ['CODEX_ACQUISITION_ID', 'HE_COVERSLIP_ID', 'X', 'Y']
        other_cols = [col for col in codex_df.columns if col not in required_cols + BIOMARKERS]
        
        final_cols = required_cols + BIOMARKERS + other_cols
        codex_df = codex_df[final_cols]
        
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ: {len(codex_df)} è¡Œ Ã— {len(codex_df.columns)} åˆ—")
        return codex_df
    
    def create_metadata_dict(self, cell_df: pd.DataFrame, svs_to_uuid: Dict[str, str]) -> Dict:
        """
        åˆ›å»ºå…ƒæ•°æ®å­—å…¸
        
        Args:
            cell_df: ç»†èƒæ•°æ®DataFrame
            svs_to_uuid: SVSåˆ°UUIDçš„æ˜ å°„
            
        Returns:
            å…ƒæ•°æ®å­—å…¸
        """
        print("ğŸ—‚ï¸ åˆ›å»ºå…ƒæ•°æ®å­—å…¸...")
        
        metadata_dict = {
            "all_biomarkers": BIOMARKERS.copy()
        }
        
        # ä¸ºæ¯ä¸ªCODEX_ACQUISITION_IDåˆ›å»ºæ˜ å°„
        for acq_id in cell_df['CODEX_ACQUISITION_ID'].unique():
            # è·å–è¯¥è·å–IDçš„æ ·æœ¬ä¿¡æ¯
            sample_data = cell_df[cell_df['CODEX_ACQUISITION_ID'] == acq_id].iloc[0]
            
            # åˆ†é…H&Eå›¾åƒUUID (ç®€åŒ–æ˜ å°„)
            if svs_to_uuid:
                region_uuid = list(svs_to_uuid.values())[hash(acq_id) % len(svs_to_uuid)]
            else:
                region_uuid = str(uuid.uuid4())
            
            metadata_dict[acq_id] = {
                "HE_REGION_UUID": region_uuid,
                "study_name": sample_data.get('study_name', 'Unknown'),
                "tissue_type": sample_data.get('tissue_type', 'Unknown'),
                "biomarkers_available": [
                    marker for marker in BIOMARKERS 
                    if not pd.isna(sample_data.get(marker, np.nan))
                ]
            }
        
        print(f"âœ… å…ƒæ•°æ®åˆ›å»ºå®Œæˆ: {len(metadata_dict)-1} ä¸ªè·å–ID")
        return metadata_dict
    
    def create_dataset_splits(self, metadata_dict: Dict) -> Dict:
        """
        åˆ›å»ºæ•°æ®é›†åˆ’åˆ†
        
        Args:
            metadata_dict: å…ƒæ•°æ®å­—å…¸
            
        Returns:
            æ•°æ®é›†åˆ’åˆ†å­—å…¸
        """
        print("ğŸ“Š åˆ›å»ºæ•°æ®é›†åˆ’åˆ†...")
        
        acq_ids = [key for key in metadata_dict.keys() if key != "all_biomarkers"]
        np.random.shuffle(acq_ids)
        
        n_total = len(acq_ids)
        n_train = int(n_total * 0.7)
        n_val = int(n_total * 0.15)
        
        splits = {
            "train": acq_ids[:n_train],
            "val": acq_ids[n_train:n_train+n_val],
            "test": acq_ids[n_train+n_val:]
        }
        
        print(f"âœ… æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒ{len(splits['train'])} | éªŒè¯{len(splits['val'])} | æµ‹è¯•{len(splits['test'])}")
        return splits
    
    def save_dataset(self, cell_df: pd.DataFrame, metadata_dict: Dict, dataset_splits: Dict):
        """
        ä¿å­˜å®Œæ•´æ•°æ®é›†
        
        Args:
            cell_df: ç»†èƒæ•°æ®
            metadata_dict: å…ƒæ•°æ®å­—å…¸
            dataset_splits: æ•°æ®é›†åˆ’åˆ†
        """
        print("ğŸ’¾ ä¿å­˜æ•°æ®é›†æ–‡ä»¶...")
        
        # ä¿å­˜ä¸»æ•°æ®æ–‡ä»¶
        cell_data_path = self.output_dir / "data" / "cell_measurements.pqt"
        cell_df.to_parquet(cell_data_path, engine='fastparquet', compression='snappy')
        print(f"âœ… ä¿å­˜: {cell_data_path} ({cell_data_path.stat().st_size / 1024 / 1024:.1f} MB)")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_path = self.output_dir / "metadata" / "metadata_dict.pkl"
        with open(metadata_path, 'wb') as f:
            pickle.dump(metadata_dict, f, protocol=4)
        print(f"âœ… ä¿å­˜: {metadata_path} ({metadata_path.stat().st_size / 1024:.1f} KB)")
        
        # ä¿å­˜æ•°æ®é›†åˆ’åˆ†
        splits_path = self.output_dir / "metadata" / "dataset_splits.json"
        with open(splits_path, 'w') as f:
            json.dump(dataset_splits, f, indent=2)
        print(f"âœ… ä¿å­˜: {splits_path}")
        
        # ç”Ÿæˆæ•°æ®é›†æ‘˜è¦
        self._generate_summary(cell_df, metadata_dict, dataset_splits)
    
    def _generate_summary(self, cell_df: pd.DataFrame, metadata_dict: Dict, dataset_splits: Dict):
        """ç”Ÿæˆæ•°æ®é›†æ‘˜è¦æŠ¥å‘Š"""
        summary_path = self.output_dir / "dataset_summary.txt"
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("ROSIEæ•°æ®é›†æ‘˜è¦æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            # åŸºæœ¬ç»Ÿè®¡
            f.write(f"ç»†èƒæ€»æ•°: {len(cell_df):,}\n")
            f.write(f"è·å–IDæ•°: {len(metadata_dict) - 1}\n")
            f.write(f"ç”Ÿç‰©æ ‡è®°ç‰©æ•°: {len(BIOMARKERS)}\n\n")
            
            # æ•°æ®é›†åˆ’åˆ†
            f.write("æ•°æ®é›†åˆ’åˆ†:\n")
            for split, ids in dataset_splits.items():
                f.write(f"  {split}: {len(ids)} ä¸ªè·å–ID\n")
            f.write("\n")
            
            # ç»„ç»‡ç±»å‹åˆ†å¸ƒ
            if 'tissue_type' in cell_df.columns:
                tissue_counts = cell_df['tissue_type'].value_counts()
                f.write("ç»„ç»‡ç±»å‹åˆ†å¸ƒ:\n")
                for tissue, count in tissue_counts.items():
                    f.write(f"  {tissue}: {count:,} ä¸ªç»†èƒ\n")
                f.write("\n")
            
            # ç”Ÿç‰©æ ‡è®°ç‰©ç»Ÿè®¡
            f.write("ç”Ÿç‰©æ ‡è®°ç‰©è¡¨è¾¾ç»Ÿè®¡:\n")
            for marker in BIOMARKERS[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                if marker in cell_df.columns:
                    valid_count = cell_df[marker].notna().sum()
                    mean_expr = cell_df[marker].mean()
                    f.write(f"  {marker}: {valid_count:,} ä¸ªæœ‰æ•ˆå€¼, å¹³å‡è¡¨è¾¾ {mean_expr:.3f}\n")
            f.write("  ...\n")
        
        print(f"âœ… ç”Ÿæˆæ‘˜è¦: {summary_path}")
    
    def validate_dataset(self) -> bool:
        """éªŒè¯æ•°æ®é›†å®Œæ•´æ€§"""
        print("ğŸ” éªŒè¯æ•°æ®é›†...")
        
        required_files = [
            self.output_dir / "data" / "cell_measurements.pqt",
            self.output_dir / "metadata" / "metadata_dict.pkl",
            self.output_dir / "metadata" / "dataset_splits.json"
        ]
        
        for file_path in required_files:
            if not file_path.exists():
                print(f"âŒ ç¼ºå°‘æ–‡ä»¶: {file_path}")
                return False
        
        try:
            # éªŒè¯æ•°æ®æ–‡ä»¶
            df = pd.read_parquet(required_files[0])
            required_cols = ['CODEX_ACQUISITION_ID', 'HE_COVERSLIP_ID', 'X', 'Y'] + BIOMARKERS
            
            for col in required_cols:
                if col not in df.columns:
                    print(f"âŒ ç¼ºå°‘åˆ—: {col}")
                    return False
            
            # éªŒè¯å…ƒæ•°æ®
            with open(required_files[1], 'rb') as f:
                metadata = pickle.load(f)
            
            if "all_biomarkers" not in metadata:
                print("âŒ å…ƒæ•°æ®ç¼ºå°‘ all_biomarkers")
                return False
            
            # éªŒè¯å›¾åƒç›®å½•
            images_dir = self.output_dir / "images"
            if not images_dir.exists():
                print("âŒ ç¼ºå°‘å›¾åƒç›®å½•")
                return False
            
            print("âœ… æ•°æ®é›†éªŒè¯é€šè¿‡!")
            return True
            
        except Exception as e:
            print(f"âŒ éªŒè¯å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ROSIEæ•°æ®é›†ç”Ÿæˆå™¨")
    parser.add_argument("--codex-dir", required=True, help="CODEXæ•°æ®ç›®å½•")
    parser.add_argument("--svs-dir", required=True, help="SVSæ–‡ä»¶ç›®å½•")
    parser.add_argument("--output-dir", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--mock-data", action="store_true", help="ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹ç”ŸæˆROSIEæ•°æ®é›†")
    print(f"CODEXç›®å½•: {args.codex_dir}")
    print(f"SVSç›®å½•: {args.svs_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print("-" * 50)
    
    # åˆ›å»ºæ•°æ®é›†ç”Ÿæˆå™¨
    generator = ROSIEDatasetGenerator(args.output_dir)
    
    try:
        # æ­¥éª¤1: è½¬æ¢SVSåˆ°Zarr
        svs_to_uuid = generator.convert_svs_to_zarr(args.svs_dir)
        
        # æ­¥éª¤2: åŠ è½½CODEXæ•°æ®
        codex_df = generator.load_codex_data(args.codex_dir)
        
        # æ­¥éª¤3: å¤„ç†ç»†èƒæ•°æ®
        cell_df = generator.process_cell_data(codex_df, svs_to_uuid)
        
        # æ­¥éª¤4: åˆ›å»ºå…ƒæ•°æ®
        metadata_dict = generator.create_metadata_dict(cell_df, svs_to_uuid)
        
        # æ­¥éª¤5: åˆ›å»ºæ•°æ®é›†åˆ’åˆ†
        dataset_splits = generator.create_dataset_splits(metadata_dict)
        
        # æ­¥éª¤6: ä¿å­˜æ•°æ®é›†
        generator.save_dataset(cell_df, metadata_dict, dataset_splits)
        
        # æ­¥éª¤7: éªŒè¯æ•°æ®é›†
        if generator.validate_dataset():
            print("\nğŸ‰ æ•°æ®é›†ç”ŸæˆæˆåŠŸ!")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
            print("ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨train.pyå¼€å§‹è®­ç»ƒäº†")
        else:
            print("\nâŒ æ•°æ®é›†éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç”Ÿæˆè¿‡ç¨‹")
            
    except Exception as e:
        print(f"\nâŒ æ•°æ®é›†ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
